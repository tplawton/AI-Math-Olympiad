{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":73231,"databundleVersionId":8365361,"sourceType":"competition"},{"sourceId":7369493,"sourceType":"datasetVersion","datasetId":4281572},{"sourceId":8012825,"sourceType":"datasetVersion","datasetId":4720595},{"sourceId":8023365,"sourceType":"datasetVersion","datasetId":4728129},{"sourceId":8052555,"sourceType":"datasetVersion","datasetId":4748944},{"sourceId":11261,"sourceType":"modelInstanceVersion","modelInstanceId":8332},{"sourceId":11264,"sourceType":"modelInstanceVersion","modelInstanceId":8318}],"dockerImageVersionId":30699,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":724.728315,"end_time":"2024-02-29T09:37:08.760349","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-29T09:25:04.032034","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"21267b653022419eb6fc3f47aa4db8ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_926e7ccdad6440be85c76931860b744c","placeholder":"​","style":"IPY_MODEL_feef8334edb24f6da22e8bb1d8d80c67","value":"Loading checkpoint shards: 100%"}},"2144e851698b4707ad1c7fc29fe21b03":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3963993becfa487c9ff725f211915e67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7a725e1b0cc4ad78a62beab5f663065","placeholder":"​","style":"IPY_MODEL_fdb32baaed7145d8a8024b615ef242ca","value":" 19/19 [10:48&lt;00:00, 33.24s/it]"}},"5882b6e860be4a0db012a64fc0704a3f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_21267b653022419eb6fc3f47aa4db8ed","IPY_MODEL_d91eb83d016a4381828192a98f798f9b","IPY_MODEL_3963993becfa487c9ff725f211915e67"],"layout":"IPY_MODEL_6a892a5561f742bb9db9f13859c18e90"}},"6a892a5561f742bb9db9f13859c18e90":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"926e7ccdad6440be85c76931860b744c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d91eb83d016a4381828192a98f798f9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2144e851698b4707ad1c7fc29fe21b03","max":19,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e0693b32889c42b18b9a3844e045d048","value":19}},"e0693b32889c42b18b9a3844e045d048":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f7a725e1b0cc4ad78a62beab5f663065":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdb32baaed7145d8a8024b615ef242ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"feef8334edb24f6da22e8bb1d8d80c67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"TPLAWTON AIMO COMPETITION\n\nForked from: https://www.kaggle.com/code/suzchin/solution-baseline-deepseekmath-7b and fine-tuned\n\nDeepseek-Math: https://github.com/deepseek-ai/DeepSeek-Math\n\nDATASETS LOADED IN ENVIRONMENT (Some not used):  \nAIMO  \ndeepseek-math  \naccelerate  \nbitsandbytes  \nopen-math-mistral  \ngemma 7b-it  \ngemma 2b-it  \n\nThis notebook utilizes the DeepseekMath model to predict answers to complex math problems formatted in Latex. The model generates and executes code to solve these problems, then extracts and processes the numeric answers from the output. Key techniques include self-consistency for best result selection, custom stopping criteria for iterative generation, prompt engineering, and debugging and time logging to ensure reproducibility and efficient runtime.","metadata":{}},{"cell_type":"code","source":"#Initialize time tracking and debugging variables\nimport time\nNOTEBOOK_START_TIME = time.time()\n\nDEBUG = False\n\n# Use Past keys for iterative text generation in prediction function\nUSE_PAST_KEY = True","metadata":{"execution":{"iopub.status.busy":"2024-06-30T20:01:59.283278Z","iopub.execute_input":"2024-06-30T20:01:59.283560Z","iopub.status.idle":"2024-06-30T20:01:59.294669Z","shell.execute_reply.started":"2024-06-30T20:01:59.283535Z","shell.execute_reply":"2024-06-30T20:01:59.293766Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Imports and Initializations\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nimport gc\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\nimport re\nimport sys\nimport subprocess\nimport math\nimport random\nfrom collections import Counter\nfrom numpy.random import choice\nimport numpy as np\n\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer, \n    AutoConfig,\n    StoppingCriteria,\n    StoppingCriteriaList,\n    set_seed\n)\nimport transformers\n\nset_seed(42)\n\n# Total number of responses generated by model, will select best answer after\nn_repetitions = 17\nTOTAL_TOKENS = 2048\n\n\nMODEL_PATH = \"/kaggle/input/deepseek-math\"\n#MODEL_PATH = \"/kaggle/input/deepseek-math/\"","metadata":{"papermill":{"duration":18.075198,"end_time":"2024-02-29T09:25:25.295954","exception":false,"start_time":"2024-02-29T09:25:07.220756","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-26T20:54:18.845252Z","iopub.execute_input":"2024-06-26T20:54:18.846118Z","iopub.status.idle":"2024-06-26T20:54:33.841817Z","shell.execute_reply.started":"2024-06-26T20:54:18.846077Z","shell.execute_reply":"2024-06-26T20:54:33.840965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract numerical substring from strings generated by model\ndef naive_parse(answer):\n    out = []\n    start = False\n    end = False\n    for l in reversed(list(answer)):\n        if l in '0123456789' and not end:\n            start = True\n            out.append(l)\n        else:\n            if start:\n                end = True\n        \n    out = reversed(out)\n    return ''.join(out)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T20:54:53.056598Z","iopub.execute_input":"2024-06-26T20:54:53.057246Z","iopub.status.idle":"2024-06-26T20:54:53.063193Z","shell.execute_reply.started":"2024-06-26T20:54:53.057214Z","shell.execute_reply":"2024-06-26T20:54:53.062267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract last line of output\ndef return_last_print(output, n):\n    lines = output.strip().split('\\n')\n    if lines:\n        return lines[n]\n    else:\n        return \"\"\n\n# take the code given by the model and process it to return its output\ndef process_code(code, return_shell_output=False):\n    \n    def repl(match):\n        if \"real\" not in match.group():\n            return \"{}{}\".format(match.group()[:-1], ', real=True)')\n        else:\n            return \"{}{}\".format(match.group()[:-1], ')')\n    code = re.sub(r\"symbols\\([^)]+\\)\", repl, code)\n\n    if return_shell_output:\n        code = code.replace('\\n', '\\n    ')\n            # Add a try...except block\n        code = \"\\ntry:\\n    from sympy import *\\n{}\\nexcept Exception as e:\\n    print(e)\\n    print('FAIL')\\n\".format(code)\n    \n    if not return_shell_output:\n        print(code)\n    with open('code.py', 'w') as fout:\n        fout.write(code)\n    \n    batcmd = 'timeout 7 ' + sys.executable + ' code.py'\n    try:\n        shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n        return_value = return_last_print(shell_output, -1)\n        print(shell_output)\n        if return_shell_output:\n            if return_value=='FAIL':\n                CODE_STATUS = False\n                return_value = return_last_print(shell_output, -2)\n                if \"not defined\" in return_value:\n                    return_value+='\\nTry checking the formatting and imports'\n            else:\n                CODE_STATUS = True\n            return return_value, CODE_STATUS  \n        code_output = round(float(eval(return_value))) % 1000\n    except Exception as e:\n        print(e,'shell_output')\n        code_output = -1\n    \n    if return_shell_output:\n        if code_output==-1:\n            CODE_STATUS = False\n        else:\n            CODE_STATUS = True\n        return code_output, CODE_STATUS  \n    \n    return code_output\n\n# process output to return just numeric answer modulo 1000\ndef process_text_output(output):\n    result = output    \n    try:\n        result_output = re.findall(r'\\\\boxed\\{(\\d+)\\}', result)\n\n        print('BOXED', result_output)\n        if not len(result_output):\n            result_output = naive_parse(result)\n        else:\n            result_output = result_output[-1]\n\n        print('BOXED FINAL', result_output)\n        if not len(result_output):\n            result_output = -1\n        \n        else:\n            result_output = round(float(eval(result_output))) % 1000\n    \n    except Exception as e:\n        print(e)\n        print('ERROR PARSING TEXT')\n        result_output = -1\n    \n    return result_output\n","metadata":{"execution":{"iopub.status.busy":"2024-06-26T20:54:54.979177Z","iopub.execute_input":"2024-06-26T20:54:54.979535Z","iopub.status.idle":"2024-06-26T20:54:54.993845Z","shell.execute_reply.started":"2024-06-26T20:54:54.979506Z","shell.execute_reply":"2024-06-26T20:54:54.992958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Empty cuda cache \ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-06-26T20:54:58.188162Z","iopub.execute_input":"2024-06-26T20:54:58.188851Z","iopub.status.idle":"2024-06-26T20:54:58.420637Z","shell.execute_reply.started":"2024-06-26T20:54:58.188816Z","shell.execute_reply":"2024-06-26T20:54:58.419647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configurate and load Deepseek Math Model and Tokenizers\nconfig = AutoConfig.from_pretrained(MODEL_PATH)\nconfig.gradient_checkpointing = True\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n\ndevice_map = [('model.embed_tokens', 0),\n             ('model.layers.0', 0),\n             ('model.layers.1', 0),\n             ('model.layers.2', 0),\n             ('model.layers.3', 0),\n             ('model.layers.4', 0),\n             ('model.layers.5', 0),\n             ('model.layers.6', 0),\n             ('model.layers.7', 0),\n             ('model.layers.8', 0),\n             ('model.layers.9', 0),\n             ('model.layers.10', 0),\n             ('model.layers.11', 0),\n             ('model.layers.12', 0),\n             ('model.layers.13', 0),\n             ('model.layers.14', 0),\n             ('model.layers.15', 0),\n             ('model.layers.16', 1),\n             ('model.layers.17', 1),\n             ('model.layers.18', 1),\n             ('model.layers.19', 1),\n             ('model.layers.20', 1),\n             ('model.layers.21', 1),\n             ('model.layers.22', 1),\n             ('model.layers.23', 1),\n             ('model.layers.24', 1),\n             ('model.layers.25', 1),\n             ('model.layers.26', 1),\n             ('model.layers.27', 1),\n             ('model.layers.28', 1),\n             ('model.layers.29', 1),\n             ('model.norm', 1),\n             ('lm_head', 1)]\n\ndevice_map = {ii:jj for (ii,jj) in device_map}\n \nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_PATH,\n    device_map=device_map,\n    torch_dtype=\"auto\",\n    trust_remote_code=True,\n    config=config\n)\n\n# Helper function for custom stopping criteria in the text generation\nclass StoppingCriteriaSub(StoppingCriteria):\n    def __init__(self, stops = [], encounters=1):\n        super().__init__()\n        self.stops = [stop.to(\"cuda\") for stop in stops]\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n        for stop in self.stops:\n            last_token = input_ids[0][-len(stop):]\n            if torch.all(torch.eq(stop,last_token)):\n                return True\n        return False\n\n\n# Custom Stop words for iterative generation\nstop_words = [\"```output\", \"```python\", \"```\\nOutput\" , \")\\n```\" , \"``````output\"]   \nstop_words_ids = [tokenizer(stop_word, return_tensors='pt', add_special_tokens=False)['input_ids'].squeeze() for stop_word in stop_words]\nstopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])\n\nmodel.dtype, model.hf_device_map","metadata":{"execution":{"iopub.status.busy":"2024-06-26T20:55:00.979384Z","iopub.execute_input":"2024-06-26T20:55:00.979746Z","iopub.status.idle":"2024-06-26T20:58:17.373020Z","shell.execute_reply.started":"2024-06-26T20:55:00.979715Z","shell.execute_reply":"2024-06-26T20:58:17.372095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PROMPT ENGINEERING: 1 OF 2 SEPARATE PROMPTS WILL BE CHOSEN FOR EACH REPITION \ncode = \"\"\"Below is a math problem you are to solve (positive numerical answer):\n\\\"{}\\\"\nTo accomplish this, first determine a sympy-based approach for solving the problem by \nlisting each step to take and what functions need to be called in each step. Be clear \nso even an idiot can follow your instructions, and remember, your final answer should \nbe positive integer, not an algebraic expression!\nWrite the entire script covering all the steps (use comments and document it well) and \n\\print the result. After solving the problem, output the final numerical answer within \\\\boxed{}.\n\nApproach:\"\"\"\n\n\ncot = \"\"\"Below is a math problem you are to solve (positive numerical answer!):\n\\\"{}\\\"\nAnalyze this problem and think step by step to come to a solution with programs. \nAfter solving the problem, output the final numerical answer within \\\\boxed{}.\\n\\n\"\"\"\n\npromplt_options = [code,cot]","metadata":{"execution":{"iopub.status.busy":"2024-06-26T20:58:29.649020Z","iopub.execute_input":"2024-06-26T20:58:29.650017Z","iopub.status.idle":"2024-06-26T20:58:29.655986Z","shell.execute_reply.started":"2024-06-26T20:58:29.649975Z","shell.execute_reply":"2024-06-26T20:58:29.654937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# seed for reproducability\ndef seed_everything(seed):\n    import random, os\n    import numpy as np\n    import torch\n    \n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    set_seed(seed)\n    \nseed_everything(42)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T20:58:31.628350Z","iopub.execute_input":"2024-06-26T20:58:31.629029Z","iopub.status.idle":"2024-06-26T20:58:31.635469Z","shell.execute_reply.started":"2024-06-26T20:58:31.628994Z","shell.execute_reply":"2024-06-26T20:58:31.634612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Initializew Model parameters\ntemperature = 0.89645\ntop_p = 1.0\ntemperature_coding = 0.89645\ntop_p_coding = 1.0\n\n# Initialize lists for prediction function\ntotal_results = {}\ntotal_answers = {}\nbest_stats = {}\ntotal_outputs = {}\nquestion_type_counts = {}\nstarting_counts = (2,3)","metadata":{"papermill":{"duration":34.259365,"end_time":"2024-02-29T09:37:05.548829","exception":false,"start_time":"2024-02-29T09:36:31.289464","status":"completed"},"tags":[],"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-06-26T20:58:33.597855Z","iopub.execute_input":"2024-06-26T20:58:33.598483Z","iopub.status.idle":"2024-06-26T20:58:33.604217Z","shell.execute_reply.started":"2024-06-26T20:58:33.598451Z","shell.execute_reply":"2024-06-26T20:58:33.603273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PREDICTION FUNCTION\n\nextra_time = 0\nallowed = 0\ndef predict(problem):\n    seed_everything(69)\n    \n    #Initialize parameters and lists for prediction function\n    temperature = 0.9\n    top_p = 1.0\n    temperature_coding = 0.9\n    top_p_coding = 1.0\n\n    total_results = {}\n    total_answers = {}\n    best_stats = {}\n    total_outputs = {}\n    question_type_counts = {}\n    starting_counts = (2,3)\n    i = 0\n    \n    global n_repetitions,TOTAL_TOKENS,model,tokenizer,USE_PAST_KEY,NOTEBOOK_START_TIME,promplt_options,code,cot,extra_time,allowed\n    \n    # If notebook running too long, return 0 to avoid running indefinitely\n    if time.time()-NOTEBOOK_START_TIME>=32200:\n        return 0\n    \n    #Start time\n    PROBLEM_START_TIME = time.time()\n    allowed = min(extra_time,600)\n    \n    #MAIN LOOP \n    for jj in tqdm(range(n_repetitions)):\n        \n        # Early Exit Conditions (found best, over allowed runtime)\n        best, best_count = best_stats.get(i,(-1,-1))\n        if best_count>np.sqrt(jj):\n            print(\"SKIPPING CAUSE ALREADY FOUND BEST\")\n            continue\n        \n        \n        if time.time()-PROBLEM_START_TIME>=720+allowed:\n            \n            extra = (time.time()-PROBLEM_START_TIME)-720 \n            extra_time-=max(0,extra)\n            \n            return best_stats[0][0]\n        \n        if time.time()-NOTEBOOK_START_TIME>=32200:\n            return best_stats[0][0]\n\n        outputs = total_outputs.get(i,[])\n        text_answers, code_answers = question_type_counts.get(i,starting_counts)\n        results = total_results.get(i,[])\n        answers = total_answers.get(i,[])  \n        \n        for _ in range(5):\n            torch.cuda.empty_cache()\n            gc.collect()\n            time.sleep(0.2)\n        \n        try:\n            #initialize variables\n            ALREADY_GEN = 0\n            code_error = None\n            code_error_count = 0\n            code_output = -1\n            counts = np.array([text_answers,code_answers])\n            \n            # draw either the code or cot prompt with p = counts/counts.sum\n            draw = choice(promplt_options, 1,\n                          p=counts/counts.sum())\n            \n            #Create prompt\n            initail_message = draw[0].format(problem,\"{}\")            \n            prompt = f\"User: {initail_message}\"\n\n            current_printed = len(prompt)\n            print(f\"{jj}_{prompt}\\n\")\n\n            model_inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n            input_len = len(model_inputs['input_ids'][0])\n\n            #Generate first section of code, storing the past_values for iterative generation\n            generation_output = model.generate(**model_inputs, \n                                               max_new_tokens=TOTAL_TOKENS-ALREADY_GEN,\n                                               return_dict_in_generate=USE_PAST_KEY,\n                                               do_sample = True,\n                                               temperature = temperature,\n                                               top_p = top_p,\n                                               num_return_sequences=1, stopping_criteria = stopping_criteria)\n            \n            if USE_PAST_KEY:\n                output_ids = generation_output.sequences[0]\n            else:\n                output_ids = generation_output[0]\n                \n            # decode output and initialize cummulative code variable\n            decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True)\n            print(f\"{decoded_output[current_printed:]}\\n\")\n            current_printed += len(decoded_output[current_printed:])\n            cummulative_code = \"\" \n            \n            stop_word_cond = False\n            for stop_word in stop_words:\n                stop_word_cond = stop_word_cond or (decoded_output[-len(stop_word):]==stop_word)\n                \n            # Continue generation until stopping criteria is met or gone through all tokens\n            while (stop_word_cond) and (ALREADY_GEN<(TOTAL_TOKENS)):\n                \n                # if code generates output, include its output in prompt, if not continue generating code\n                if (decoded_output[-len(\"```python\"):]==\"```python\"):\n                    temperature_inner=temperature_coding\n                    top_p_inner = top_p_coding\n                    prompt = decoded_output\n                else:\n                    temperature_inner=temperature\n                    top_p_inner = top_p\n                    try:\n                        if (decoded_output[-len(\"``````output\"):]==\"``````output\"):\n                            code_text = decoded_output.split('```python')[-1].split(\"``````\")[0]\n                        else:\n                            code_text = decoded_output.split('```python')[-1].split(\"```\")[0]\n                        \n                        #Add generated code to cummulative code and retrieve the output with process_code\n                        cummulative_code+=code_text\n                        code_output, CODE_STATUS = process_code(cummulative_code, return_shell_output=True)\n                        print('CODE RESULTS', code_output)\n                        \n                \n                        if code_error==code_output:\n                            code_error_count+=1\n                        else:\n                            code_error=code_output\n                            code_error_count = 0\n\n                        if not CODE_STATUS:\n                            cummulative_code = cummulative_code[:-len(code_text)]\n\n                            if code_error_count>=1:\n                                print(\"REPEATED ERRORS\")\n                                break\n\n                    except Exception as e:\n                        print(e)\n                        print('ERROR PARSING CODE')\n                        code_output = -1\n                    \n                    # Include output in prompt\n                    if code_output!=-1:\n                        if (decoded_output[-len(\")\\n```\"):]==\")\\n```\"):\n                            prompt = decoded_output+'```output\\n'+str(code_output)+'\\n```\\n'\n                        else:\n                            prompt = decoded_output+'\\n'+str(code_output)+'\\n```\\n'\n                    else:\n                        prompt = decoded_output\n                        cummulative_code=\"\"\n                        \n                model_inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n                ALREADY_GEN =  len(model_inputs['input_ids'][0])-input_len\n\n                # Continue generating with past key to prevent extra computation\n                if USE_PAST_KEY:\n                    old_values = generation_output.past_key_values\n                else:\n                    old_values = None\n                \n                # Continue generation with old values stored from the first generation and with new prompt\n                generation_output = model.generate(**model_inputs, \n                                                   max_new_tokens=TOTAL_TOKENS-ALREADY_GEN, \n                                                   return_dict_in_generate=USE_PAST_KEY,\n                                                   past_key_values=old_values,\n                                                   do_sample = True,\n                                                   temperature = temperature_inner,\n                                                   top_p = top_p_inner,\n                                                   num_return_sequences=1, stopping_criteria = stopping_criteria)\n                if USE_PAST_KEY:\n                    output_ids = generation_output.sequences[0]\n                else:\n                    output_ids = generation_output[0]\n                    \n                decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True)\n                print(f\"\\nINTERMEDIATE OUT :\\n{decoded_output[current_printed:]}\\n\")\n                current_printed+=len(decoded_output[current_printed:])\n                \n                # If done generating, then set stop_word_cond to false\n                stop_word_cond = False\n                for stop_word in stop_words:\n                    stop_word_cond = stop_word_cond or (decoded_output[-len(stop_word):]==stop_word)\n                    \n            if USE_PAST_KEY:\n                output_ids = generation_output.sequences[0]\n            else:\n                output_ids = generation_output[0]\n\n            # Get raw output from tokenizer (ignoring prompt) and then process output to obtain numerical result % 1000\n            raw_output = tokenizer.decode(output_ids[input_len:], skip_special_tokens=True)\n            #print(f\"\\n\\nOutput :\\n{raw_output}\\n\")                            \n            result_output = process_text_output(raw_output)\n            \n            try:\n                # Extra modulo 1000 for consistency\n                code_output = round(float(eval(code_output))) % 1000\n            except Exception as e:\n                print(e,'final_eval')\n                code_output = -1\n        except Exception as e:\n            print(e,\"5\")\n            result_output, code_output = -1, -1\n\n        # add code_output to outputs\n        if code_output!=-1:\n            outputs.append(code_output)\n            code_answers+=1\n\n        # add result_output to outputs\n        if result_output!=-1:\n            outputs.append(result_output)\n            text_answers+=1\n\n        # Best answer selection,take the most common answer and place in best \n        if len(outputs) > 0:\n            occurances = Counter(outputs).most_common()\n            print(occurances)\n            if occurances[0][1] > best_count:\n                print(\"GOOD ANSWER UPDATED!\")\n                best = occurances[0][0]\n                best_count = occurances[0][1]\n            if occurances[0][1] > 5:\n                print(\"ANSWER FOUND!\")\n                break\n\n        results.append(result_output)\n        answers.append(code_output)\n        \n        best_stats[i] = (best, best_count) \n        question_type_counts[i] = (text_answers, code_answers)\n        total_outputs[i] = outputs\n        \n        total_results[i] = results\n        total_answers[i] = answers\n\n        print(\"code_answers\",code_answers-starting_counts[1],\"text_answers\",text_answers-starting_counts[0])\n   \n    # After all repitions, return the most common answer\n    remaining = 720-(time.time()-PROBLEM_START_TIME)\n    remaining = max(0,remaining)\n    extra_time+=remaining\n    return best_stats[0][0]","metadata":{"execution":{"iopub.status.busy":"2024-06-26T20:58:35.566625Z","iopub.execute_input":"2024-06-26T20:58:35.567177Z","iopub.status.idle":"2024-06-26T20:58:35.602963Z","shell.execute_reply.started":"2024-06-26T20:58:35.567142Z","shell.execute_reply":"2024-06-26T20:58:35.602004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission with the new API","metadata":{}},{"cell_type":"code","source":"import aimo\n\nenv = aimo.make_env()\niter_test = env.iter_test()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for test, sample_submission in iter_test:\n    sample_submission['answer'] = predict(test['problem'].values[0])\n    env.predict(sample_submission)\n    print(test)\n    print(sample_submission)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('code.py', 'w') as fout:\n    fout.write(\"print('done')\")\n\nbatcmd = 'timeout 7 ' + sys.executable + ' code.py'\ntry:\n    shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n    print(shell_output)\nexcept:\n    pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}